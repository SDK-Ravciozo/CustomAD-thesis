{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccd9c36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from random import sample\n",
    "import matplotlib.pyplot as plt\n",
    "from thesis_library import *\n",
    "from tqdm import tqdm, trange\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def str_to_datetime(text: str):\n",
    "    try:\n",
    "        return datetime.strptime(text, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    except:\n",
    "        return datetime.strptime(text, \"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "def sample_or_all(thing, size):\n",
    "    if size>len(thing):\n",
    "        return thing\n",
    "    else:\n",
    "        return sample(thing, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbaec36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(short = True, timestamp_threshold = None) -> tuple:\n",
    "    '''\n",
    "    timestamp_threshold: if int, skips the checkins with almost-unique timestamps (less than timestamp_threshold times)\n",
    "                         e.g. if there is a single check-in on 26th January, skip it. \n",
    "                         If None, does not perform the skipping\n",
    "                         \n",
    "    returns:             df_checkins, df_edges'''\n",
    "    \n",
    "    if not short:\n",
    "        df_checkins = pd.read_table('./data/Gowalla/Gowalla_totalCheckins.txt', delimiter='\\t', header=None)\n",
    "        df_checkins.columns = ['user', 'time', 'latitude', 'longitude', 'location_id']\n",
    "        #print(type(df_checkins['time'][0]))\n",
    "        \n",
    "        df_edges = pd.read_table('./data/Gowalla/Gowalla_edges.txt', delimiter='\\t', header=None)\n",
    "        \n",
    "    else:\n",
    "        df_checkins = pd.read_csv(\"./data/Gowalla/Gowalla_totalCheckins_short.csv\")\n",
    "        df_edges = pd.read_csv(\"./data/Gowalla/Gowalla_edges_short.csv\")\n",
    "    \n",
    "    #Processing the dates and times\n",
    "    #Setting the relative time (in days) w.r.t. the first day present in the dataframe\n",
    "    df_checkins['time'] = df_checkins['time'].apply(str_to_datetime)\n",
    "    min_date = df_checkins['time'].min()\n",
    "    df_checkins['time_elapsed_days'] = df_checkins['time'] - min_date\n",
    "    df_checkins['time_elapsed_days'] = df_checkins['time_elapsed_days'].apply(lambda x: x.days)\n",
    "    \n",
    "    df_checkins.sort_values(['time'], ascending=True, inplace=True)\n",
    "    df_checkins.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    print(\"There are\", len(df_checkins['user'].unique()), \"unique users present.\")\n",
    "    print(\"There are\", len(df_checkins['location_id'].unique()), \"unique locations present.\")\n",
    "    \n",
    "    #Delete rows with time_elapsed being equal to some value that appears almost no times (once, twice etc.)\n",
    "    if type(timestamp_threshold) is int:\n",
    "        df_checkins_grouped = df_checkins.groupby('time_elapsed_days')\n",
    "        dic = dict(df_checkins_grouped.count()['user'] > timestamp_threshold)\n",
    "        dic = {key: dic[key] for key in dic if dic[key] == True}\n",
    "        df_checkins['correct_timestamps'] = df_checkins['time_elapsed_days'].apply(lambda x: x in dic)\n",
    "        df_checkins[df_checkins['correct_timestamps'] == True]\n",
    "        \n",
    "    print(\"There are\", len(df_checkins['user'].unique()), \"unique users present (after thresholding).\")\n",
    "    print(\"There are\", len(df_checkins['location_id'].unique()), \"unique locations present (after thresholding).\")\n",
    "     \n",
    "    return df_checkins, df_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ecf3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_checkins, df_edges = read_data(short=True, timestamp_threshold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f28d401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_a_friendship_graph(df_edges: pd.DataFrame, return_graph=False, max_friends=100) -> dict | nx.Graph:\n",
    "    '''\n",
    "    Given all the edges, constructs an object storing all friendship connections\n",
    "    Warning: can be huge\n",
    "    The graph version is slow af\n",
    "    '''\n",
    "    \n",
    "    #Creates a dataframe that has nodes (0-999...) as indices and a single column with a set of friends\n",
    "    df_grouped = df_edges.groupby('0').agg(set)\n",
    "    \n",
    "    #Converts the above dataframe into an nx.Graph that has deg(v) >= 100 nodes pruned out\n",
    "    if return_graph:\n",
    "        g = nx.Graph()\n",
    "        for i in df_grouped.index:\n",
    "            g.add_edges_from([(i, friend) for friend in list(df_grouped.loc[i])[0] \\\n",
    "                              if len(list(df_grouped.loc[i])[0]) < max_friends])\n",
    "            \n",
    "        g.remove_nodes_from([node for node in g.nodes if len(g[node]) > max_friends])\n",
    "        return g\n",
    "    \n",
    "    #Converts the above dataframe into a dictionary that has deg(v) >= 100 nodes (keys) pruned out\n",
    "    else:\n",
    "        dic = {key: value for key, value in zip(df_grouped.index, list(df_grouped['1'])) if len(value) < max_friends}\n",
    "        dic = {key: dic[key] for key in dic if len(dic[key]) < max_friends}\n",
    "        return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f898eb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining the dictionary of friendships\n",
    "#Pruning away all users with max_friends or more friends\n",
    "dic = create_a_friendship_graph(df_edges, return_graph=False, max_friends=15)\n",
    "df_checkins = df_checkins[df_checkins.apply(lambda x: x['user'] in dic, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0942bb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df_checkins: pd.DataFrame, dic: dict, sample_size=None):\n",
    "    '''\n",
    "    max_friends: limits the number of friends any node can have in the friendship network\n",
    "    sample_size: determines the number of edges sent for every checkin created\n",
    "    '''\n",
    "    \n",
    "    print(\"Can take at most\", len(df_checkins)*15, \" iterations.\") #taken from max_friends\n",
    "    \n",
    "    data, label = [], [0 for _ in range(len(df_checkins))]\n",
    "    \n",
    "    for login in tqdm(df_checkins.iloc):\n",
    "        \n",
    "        #If sample size is None, create an edge for EVERY friend\n",
    "        if sample_size is None:\n",
    "            friends = dic[login['user']]\n",
    "         #If sample size is an integer, take a sample of that many friends to add edges for them\n",
    "        else:\n",
    "            friends = sample_or_all(list(dic[login['user']]), sample_size)\n",
    "            \n",
    "        for friend in friends: #Skipping the weight = 1 here for data reduction\n",
    "            data.append([login['user'], friend, login['time_elapsed_days']])\n",
    "        \n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8e44aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = create_dataset(df_checkins, dic, sample_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e99052fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pickling (saving) the list to the disk:\n",
    "\n",
    "# with open('./data/Gowalla/Gowalla_short_edges_times_unplanted.txt', 'wb') as fp:\n",
    "#     pickle.dump(data, fp)\n",
    "#     #Works, just cannot be opened for human reader\n",
    "    \n",
    "with open('./data/Gowalla/Gowalla_short_edges_times_unplanted.txt', 'rb') as fp:\n",
    "    X = np.array(pickle.load(fp))\n",
    "    \n",
    "y = [0] * len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3450dc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We want 1% of the data to be anomalous, so a bit over 11192 edges to be planted.\n",
      "Since a clique has 1/2(n^2-n) edges per n vertices, we can use 19 nodes for 171 edges.\n",
      "Then, we will repeat every edge 5 times for 855 edges per clique.\n",
      "We will plant them in 16 of them for 13680 edges.\n"
     ]
    }
   ],
   "source": [
    "#Plant a handful of cliques - guide\n",
    "print(\"We want 1% of the data to be anomalous, so a bit over\", len(X)//100, \"edges to be planted.\")\n",
    "print(\"Since a clique has 1/2(n^2-n) edges per n vertices, we can use 19 nodes for\", 19*(19-1)//2, \"edges.\")\n",
    "print(\"Then, we will repeat every edge 5 times for\", 5*171, \"edges per clique.\")\n",
    "print(\"We will plant them in 16 of them for\", 16*171*5, \"edges.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdeb23eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planting an anomaly at timestamp: 114\n",
      "Planting an anomaly at timestamp: 214\n",
      "Planting an anomaly at timestamp: 289\n",
      "Planting an anomaly at timestamp: 292\n",
      "Planting an anomaly at timestamp: 309\n",
      "Planting an anomaly at timestamp: 349\n",
      "Planting an anomaly at timestamp: 359\n",
      "Planting an anomaly at timestamp: 363\n",
      "Planting an anomaly at timestamp: 414\n",
      "Planting an anomaly at timestamp: 430\n",
      "Planting an anomaly at timestamp: 449\n",
      "Planting an anomaly at timestamp: 533\n",
      "Planting an anomaly at timestamp: 550\n",
      "Planting an anomaly at timestamp: 590\n",
      "Planting an anomaly at timestamp: 591\n",
      "Planting an anomaly at timestamp: 618\n",
      "There are 1146561 edges, out of which 27325 anomaly edges.\n"
     ]
    }
   ],
   "source": [
    "plant_anomalies(X, y, dataset='Gowalla', n_imputations=16, n_vertices=19, n_repetitions=5, anomaly_type='clique')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74bc491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_uniform_data(n: int):\n",
    "    '''For each node, creates 1 edge to all other nodes and saves to pickle\n",
    "\n",
    "    n: int, the number of nodes'''\n",
    "    \n",
    "    print(\"Creating \" + str(n*(n-1)//2) + \" edges.\")\n",
    "    data = [(u, v, 1) for u in range(n) for v in range(n) if u != v]\n",
    "    label = [0] * len(data)\n",
    "    \n",
    "    #Pickling (saving) the list to the disk:\n",
    "    if 'uniform_data_' + str(n) + '.txt' not in os.listdir('./data/Uniform'):\n",
    "        with open('./data/Uniform/uniform_data_'+str(n)+'.txt', 'wb') as fp:\n",
    "            pickle.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3607ff1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 244650 edges.\n",
      "Length of the dataset: 244650\n"
     ]
    }
   ],
   "source": [
    "create_uniform_data(n=700)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
