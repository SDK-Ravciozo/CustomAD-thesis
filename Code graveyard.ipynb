{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c94894ce",
   "metadata": {},
   "source": [
    "# Finished notebook\n",
    "#### What do we learn from it?\n",
    "## 1. Functions:\n",
    "\n",
    "1) test_split_size_effect:\n",
    "\n",
    "        Splits the dataset, creates training graph, runs the LP methods on the test set\n",
    "\n",
    "        Creates ./CSV/auc_prediction_rav.csv in the process\n",
    "\n",
    "2) test_on_non_splits:\n",
    "\n",
    "        First, runs MIDAS on the entire dataset. Then, splits the dataset and updates the test sores with LP\n",
    "\n",
    "        Creates ./CSV/rav_auc_split.csv\n",
    "\n",
    "3) create_auc_clique_n:\n",
    "\n",
    "        Splits the data, constructs training graphs, imputes a clique into the test set only\n",
    "\n",
    "        Runs MIDAS and MIDAS+LP on the test set\n",
    "\n",
    "        Creates ./CSV/rav_auc_clique_'+str(n)+'.csv, where n is the number of nodes taking part in the clique\n",
    "\n",
    "\n",
    "## 2. CSVs created:\n",
    "\n",
    "- ./CSV/auc_prediction_rav.csv\n",
    "\n",
    "- ./CSV/rav_auc_split.csv\n",
    "\n",
    "- ./CSV/rav_dataset_info.csv\n",
    "\n",
    "        For each split of the dataset, lists the exact train and test sizes, as well as the anomaly train and test sizes\n",
    "\n",
    "- rav_auc_clique_50.csv, where 50 is the number of imputed anomaly edges (clique)\n",
    "\n",
    "\n",
    "## 3. Some code to prove the superiority/correctness of our approaches:\n",
    "\n",
    "- Splitting works\n",
    "\n",
    "- sum(y) is faster than y.count(1)\n",
    "\n",
    "- .intersection() or .union() are faster than nx.jaccard_coefficient\n",
    "\n",
    "## 4. Some \"edge ranking\" stuff that seems highly unnecessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4a36563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Jupyter core packages...\n",
      "IPython          : 8.12.0\n",
      "ipykernel        : 6.19.2\n",
      "ipywidgets       : 8.1.2\n",
      "jupyter_client   : 8.1.0\n",
      "jupyter_core     : 5.3.0\n",
      "jupyter_server   : 1.23.4\n",
      "jupyterlab       : not installed\n",
      "nbclient         : 0.5.13\n",
      "nbconvert        : 6.5.4\n",
      "nbformat         : 5.7.0\n",
      "notebook         : 6.5.4\n",
      "qtconsole        : not installed\n",
      "traitlets        : 5.7.1\n"
     ]
    }
   ],
   "source": [
    "!jupyter --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "warming-force",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "from tqdm import tqdm, trange\n",
    "from Midas import FilteringCore, NormalCore, RelationalCore\n",
    "from random import uniform, randint\n",
    "\n",
    "from thesis_library import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54ac114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_methods(dataset: str, test_size: float, threshold=10) -> tuple:\n",
    "#     '''\n",
    "#     Returns a triple of the AUC scores of the following methods:\n",
    "#     Common Neighbours, Jaccard Coefficient, Preferential Attachment\n",
    "    \n",
    "#     If the case imbalance in the test set is too extreme (threshold param), returns None, None, None instead\n",
    "#     '''\n",
    "    \n",
    "#     print(\"Reading data\")\n",
    "#     data, label = read_data(dataset = dataset, plant='clique')\n",
    "    \n",
    "#     print(\"Splitting data\")\n",
    "#     X_train, X_test, y_train, y_test = split(data, label, test_size=test_size)\n",
    "    \n",
    "#     print(\"Constructing the training graph (anomalies disallowed)\")\n",
    "#     G = construct_training_graph(X_train, y_train, True, False)\n",
    "    \n",
    "#     print(\"Filtering the test set\")\n",
    "#     X_test, y_test = filter_test(X_test, y_test, G)\n",
    "    \n",
    "#     #AUC / ROC is ill-defined with an extreme case imbalance\n",
    "#     if sum(y_test) > threshold: #sum(y_test) is the number of anomalous edges\n",
    "        \n",
    "#         print(\"Invoking the Link Prediction methods \\n\")\n",
    "#         print(\"Dataset length:   \", len(y_test))\n",
    "#         print(\"Anomalies present:\", sum(y_test))\n",
    "        \n",
    "#         cn = apply_lp('Common Neighbours', [1]*len(y_test), X_test, G)\n",
    "#         jc = apply_lp('Jaccard Coefficient', [1]*len(y_test), X_test, G)\n",
    "#         pa = apply_lp('Preferential Attachment', [1]*len(y_test), X_test, G)\n",
    "        \n",
    "#         cn, jc, pa = roc_auc_score(y_test, cn), roc_auc_score(y_test, jc), roc_auc_score(y_test, pa)\n",
    "        \n",
    "#         cn, jc, pa = round(cn, 4), round(jc, 4), round(pa, 4)\n",
    "        \n",
    "#         return cn, jc, pa, len(y_test), sum(y_test)\n",
    "        \n",
    "#     else:\n",
    "#         print(\"Not enough anomalous edges in the test set, only\", sum(y_test))\n",
    "#         return -1, -1, -1, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83127ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cn_auc, jc_auc, pa_auc, _, _ = evaluate_methods(dataset=\"ISCX\", test_size=0.1)\n",
    "\n",
    "# print('')\n",
    "# print(\"Common Neighbours AUC:      \", cn_auc)\n",
    "# print(\"Jaccard Coefficient AUC:    \", jc_auc)\n",
    "# print(\"Preferential Attachment AUC:\", pa_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9765b1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_split_size_effect(df = None) -> None:\n",
    "#     '''\n",
    "#     Runs the evaluate_methods code for all 1:9 up to 9:1 train-test-split settings\n",
    "#     Saves the results to a dataframe called \"auc_prediction_rav.csv\"\n",
    "#     If the dataframe has already been created, pass it as an argument to append to it\n",
    "#     Otherwise, the results may be overwritten!\n",
    "#     '''\n",
    "    \n",
    "#     if df is None:\n",
    "#         df = pd.DataFrame({\"Split (train:test)\": [], \"Dataset\": [], \"Test size (overall)\": [], \\\n",
    "#                            \"Test size (anomaly)\": [], \"Method\": [], \"AUC\": []})\n",
    "        \n",
    "#     for dataset in DATASETS:\n",
    "#         for test_size in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "            \n",
    "#             print(dataset, test_size)\n",
    "            \n",
    "#             #Just for display purposes:\n",
    "#             split_name = get_split_name(test_size)\n",
    "            \n",
    "#             #Getting the AUCs:\n",
    "#             cn_auc, jc_auc, pa_auc, n, n_anomaly = evaluate_methods(dataset=dataset, test_size=test_size)\n",
    "            \n",
    "#             #Saving the results into the resul dataframe:\n",
    "#             df.loc[df.shape[0]] = [split_name, dataset, n, n_anomaly, \"Common neighbours\", cn_auc]\n",
    "#             df.loc[df.shape[0]] = [split_name, dataset, n, n_anomaly, \"Jaccard coefficient\", jc_auc]\n",
    "#             df.loc[df.shape[0]] = [split_name, dataset, n, n_anomaly, \"Preferential attachment\", pa_auc]\n",
    "            \n",
    "#             #Saving the intermediate results and cooling down the processor:\n",
    "#             df.to_csv(\"./CSV/auc_prediction_rav.csv\", index=False)\n",
    "#             time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba93990e",
   "metadata": {},
   "source": [
    "### The pipeline of the above for a different test_size split (runs 3 hours):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a3da8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"./CSV/auc_prediction_rav.csv\")\n",
    "# test_split_size_effect(df=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b1afbb",
   "metadata": {},
   "source": [
    "### Obtaining the score data for pure Preferential Attachment for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e566fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2204c656",
   "metadata": {},
   "source": [
    "### Create dataset info has simply this single line to invoke:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76861d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_dataset_info(DATASETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630d29c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def impute_clique(X_test, y_test, n: int) -> tuple:\n",
    "#     '''Perhaps it would be wiser to allow new nodes here?\n",
    "#     Imputing a clique of 0 nodes equals to not doing anything at all! Use it!'''\n",
    "    \n",
    "#     final_t = X_test[-1][-1]+1\n",
    "#     for i in range(n): #is starting from 0 okay?\n",
    "#         for j in range(n):\n",
    "#             if i != j:\n",
    "#                 X_test.append([i, j, final_t])\n",
    "#                 y_test.append(1)\n",
    "            \n",
    "#     return X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77653846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_auc_clique_n(n: int, midas_list: list, datasets: list, lp_methods: list, df_clique=None):\n",
    "#     '''\n",
    "#     1) Splits the data\n",
    "#     2) Constructs training graph on the train split\n",
    "#     3) Imputes an n-clique into the test data (pass n=0 for no imputation)\n",
    "#     4) Runs MIDAS score on the test set\n",
    "#     5) Updates it with Link Prediction scores\n",
    "#     '''\n",
    "    \n",
    "#     assert midas_version in [\"normal\", 'relational'], \"Use 'normal' or 'relational'\"\n",
    "    \n",
    "#     if df_clique is None:\n",
    "#         df_clique = pd.DataFrame(columns=[\"Split (train:test)\", \"Dataset\", \"Midas_version\", \"Method\", \"AUC\", 'Time'])\n",
    "    \n",
    "#     print(\"Trying a total of\", 9*len(midas_list)*len(datasets)*(len(lp_methods)+1), \" combinations.\")\n",
    "    \n",
    "#     ####### MAIN LOOP ###########\n",
    "#     for dataset in datasets:\n",
    "#         print(\"Reading dataset:\", dataset)\n",
    "#         X, y = read_data(dataset)\n",
    "        \n",
    "#         #Handling the proper MIDAS version and its display name:\n",
    "#         for midas_name in midas_list:\n",
    "#             if midas_name == 'normal':\n",
    "#                 midas, midas_name = NormalCore(2, 1024), 'MIDAS'\n",
    "#             elif midas_name == \"relational\":\n",
    "#                 midas, midas_name = RelationalCore(2, 1024), 'MIDAS-R'\n",
    "#             else:\n",
    "#                 raise ValueError(\"MIDAS version not supported. Pass 'normal' or 'relational'.\")\n",
    "            \n",
    "#             #Handling the proper train-test split size:\n",
    "#             for test_size in [round(0.1*(i+1), 2) for i in range(0, 9)]:\n",
    "\n",
    "#                 #Splitting the data:\n",
    "#                 X_train, X_test, y_train, y_test = split(X, y, test_size)\n",
    "\n",
    "#                 G = construct_training_graph(X_train, y_train)\n",
    "\n",
    "#                 X_test, y_test = impute_clique(X_test, y_test, n) #not too fast of an implementation\n",
    "\n",
    "#                 #Setting up a proper midas version and its display name for the CSV filename:\n",
    "#                 if midas_version == 'normal':\n",
    "#                     midas, midas_version = NormalCore(2, 1024), \"MIDAS\"\n",
    "#                 elif midas_version == 'relational':\n",
    "#                     midas, midas_version = RelationalCore(2, 1024), \"MIDAS-R\"\n",
    "\n",
    "#                 #Obtaining the results for pure MIDAS:\n",
    "#                 score = [0.0] * len(y_test)\n",
    "#                 t1 = time.time()\n",
    "\n",
    "#                 for i in trange(len(y_test), desc=midas.nameAlg, unit_scale=True):\n",
    "#                     score[i] = midas.Call(*X_test[i])\n",
    "#                     #score[i] = midas.process_edge(*X_test[i])\n",
    "\n",
    "#                 t2 = time.time()\n",
    "#                 auc = roc_auc_score(y_test, score)\n",
    "\n",
    "#                 df_clique.loc[df_clique.shape[0]] = [get_split_name(test_size), dataset, midas_version, \\\n",
    "#                                                      'none', auc, round(t2-t1, 4)]\n",
    "\n",
    "#                 #Reinforcing the scores with LP methods:\n",
    "#                 for method in lp_methods:\n",
    "#                     t1 = time.time()\n",
    "#                     method_score = apply_lp(method, score, X_test, G)\n",
    "#                     auc = roc_auc_score(y_test, method_score)\n",
    "#                     t2 = time.time()\n",
    "#                     df_clique.loc[df_clique.shape[0]] = [get_split_name(test_size), dataset, midas_version, \\\n",
    "#                                                          method, auc, round(t2-t1, 4)]\n",
    "\n",
    "#                 #Saving all the results:\n",
    "#                 df_clique.to_csv('./CSV/rav_auc_clique_'+str(n)+'.csv', index=False)\n",
    "#                 time.sleep(15)\n",
    "\n",
    "#             time.sleep(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df1332e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('./CSV/rav_auc_clique_30.csv')\n",
    "# create_acc_clique_n(0, ['normal', 'relational'], DATASETS, LP_METHODS, df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee2b382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_on_non_splits(midas_list: list, datasets: list, lp_methods: list, df=None) -> None:\n",
    "#     '''First, runs a given MIDAS on the ENTIRE dataset\n",
    "#     Then, splits the scores alongside the data to update them with LP scores\n",
    "#     If no dataframe is passed, a new one is created and saved at ./CSV/rav_auc_split.csv'''\n",
    "    \n",
    "#     print(\"Trying a total of\", 9*len(midas_list)*len(datasets)*(len(lp_methods)+1), \"combinations.\")\n",
    "    \n",
    "#     if df is None:\n",
    "#         df = pd.DataFrame(columns=['Split (train:test)', 'Dataset', '#nodes', '#edges', 'MIDAS', \n",
    "#                                    'Method', 'AUC', 'Time [s]'])\n",
    "    \n",
    "#     #Handling the dataset loop:\n",
    "#     for dataset in datasets:\n",
    "#         print(\"Reading dataset\", dataset)\n",
    "#         X, y = read_data(dataset)\n",
    "        \n",
    "#         #Handling the MIDAS loop:\n",
    "#         for midas_name in midas_list:\n",
    "#             if midas_name == 'normal':\n",
    "#                 midas, midas_name = NormalCore(2, 1024), 'MIDAS'\n",
    "#             elif midas_name == \"relational\":\n",
    "#                 midas, midas_name = RelationalCore(2, 1024), 'MIDAS-R'\n",
    "#             elif midas_name == 'custom':\n",
    "#                 print(\"Setting up the custom MIDAS.\")\n",
    "#                 midas, midas_name = MIDAS(4, 5, 3, is_switchboard=False), \"Custom\"\n",
    "#             else:\n",
    "#                 raise ValueError(\"MIDAS version not supported. Pass 'normal', 'relational', 'custom'.\")\n",
    "            \n",
    "#             #Running MIDAS on the entire dataset - the scores will be split\n",
    "#             score = [0.0] * len(y)\n",
    "#             for i in trange(len(y), desc=midas.nameAlg, unit_scale=True):\n",
    "#                 score[i] = midas.Call(*X[i])\n",
    "#                 #score[i] = midas.process_edge(*X[i])\n",
    "#             auc = roc_auc_score(y, score)\n",
    "            \n",
    "#             #Saving the MIDAS score anyway:\n",
    "#             df.loc[df.shape[0]] = [\"10:00\", dataset, -1, -1, midas_name, 'none', auc, -1]\n",
    "#             df.to_csv('./CSV/rav_auc_split.csv', index=False)\n",
    "            \n",
    "#             for test_size in [round(0.1*(i+1), 2) for i in range(0, 9)]:\n",
    "                \n",
    "#                 print(\"Processing\", get_split_name(test_size))\n",
    "                \n",
    "#                 #Preparing the split:\n",
    "#                 X_train, X_test, y_train, y_test, score_test = split(X, y, test_size, score)\n",
    "#                 G = construct_training_graph(X_train, y_train, True, False) #Not saving anomalies in\n",
    "                \n",
    "#                 #Looping over the 3 available LP methods:\n",
    "#                 for method in lp_methods: \n",
    "                    \n",
    "#                     start_time = time.time()\n",
    "#                     method_score = apply_lp(method, score_test, X_test, G)\n",
    "#                     auc = roc_auc_score(y_test, method_score) if sum(y_test) !=0 else -1\n",
    "#                     end_time = time.time()\n",
    "                    \n",
    "#                     df.loc[df.shape[0]] = [get_split_name(test_size), dataset, G.number_of_nodes(), G.number_of_edges(), \n",
    "#                                            midas_name, method, auc, round(end_time-start_time, 4)]\n",
    "                    \n",
    "#                     time.sleep(20)\n",
    "                    \n",
    "#                 df.to_csv('./CSV/rav_auc_split.csv', index=False)\n",
    "                \n",
    "#                 time.sleep(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1249334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('./CSV/rav_auc_split.csv')\n",
    "# test_on_non_splits(midas_list = ['normal', 'relational'], datasets = ['NYC_Taxi'], lp_methods=LP_METHODS, df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4751f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grand_test(midas_list: list, datasets: list, lp_methods: list, df=None) -> None:\n",
    "    '''First, runs a given MIDAS on the ENTIRE dataset\n",
    "    Then, splits the scores alongside the data to update them with LP scores\n",
    "    If no dataframe is passed, a new one is created and saved at ./CSV/rav_test_on_splits.csv'''\n",
    "    \n",
    "    print(\"Trying a total of\", 9*len(midas_list)*len(datasets)*(len(lp_methods)+1), \"combinations.\")\n",
    "    \n",
    "    if df is None:\n",
    "        df = pd.DataFrame(columns=['Split (train:test)', 'Dataset', '#nodes', '#edges', 'MIDAS', \n",
    "                                   'Method', 'AUC', 'Runtime'])\n",
    "    \n",
    "    #Handling the dataset loop:\n",
    "    for dataset in datasets:\n",
    "        print(\"Reading dataset\", dataset)\n",
    "        X, y = read_data(dataset, plant='clique')\n",
    "        \n",
    "        #Handling the MIDAS loop:\n",
    "        for midas_name in midas_list:\n",
    "            if midas_name in ['normal', 'Normal', 'MIDAS']:\n",
    "                midas, midas_name = NormalCore(2, 1024), 'MIDAS'\n",
    "            elif midas_name in ['relational', 'Relational', 'MIDAS-R']:\n",
    "                midas, midas_name = RelationalCore(2, 1024), 'MIDAS-R'\n",
    "            elif midas_name in [None, 'None', 'none']:\n",
    "                midas, midas_name = None, \"None\"\n",
    "            else:\n",
    "                raise ValueError(\"MIDAS version not supported. Pass 'normal', 'relational', 'none'.\")\n",
    "            \n",
    "            #Running MIDAS on the entire dataset - the scores will be split\n",
    "            if midas_name not in [None, 'None', 'none']:\n",
    "                score = [0.0] * len(y)\n",
    "                t1 = time.time()\n",
    "                for i in trange(len(y), desc=midas.nameAlg, unit_scale=True):\n",
    "                    score[i] = midas.Call(*X[i])\n",
    "                t2 = time.time()\n",
    "                time_taken = round(t2-t1, 4)\n",
    "                auc = roc_auc_score(y, score)\n",
    "            else:\n",
    "                score, auc, time_taken = [1.0] * len(y), -1, 0\n",
    "            \n",
    "            #Saving the MIDAS score anyway:\n",
    "            df.loc[df.shape[0]] = [\"10:00\", dataset, -1, -1, midas_name, 'None', auc, time_taken]\n",
    "            df.to_csv('./CSV/rav_test_on_splits.csv', index=False)\n",
    "            \n",
    "            for test_size in [round(0.1*(i+1), 2) for i in range(0, 9)]:\n",
    "                \n",
    "                print(\"Processing\", get_split_name(test_size))\n",
    "                \n",
    "                #Preparing the split:\n",
    "                X_train, X_test, y_train, y_test, score_test = split(X, y, test_size, score)\n",
    "                G = construct_training_graph(X_train, y_train, True, False) #Not saving anomalies in\n",
    "                \n",
    "                #Looping over the 3 available LP methods + the None LP method:\n",
    "                for method in lp_methods+['None']: \n",
    "                    \n",
    "                    t1 = time.time()\n",
    "                    method_score = apply_lp(method, score_test, X_test, G)\n",
    "                    auc = roc_auc_score(y_test, method_score) if sum(y_test) !=0 else -1\n",
    "                    t2 = time.time()\n",
    "                    \n",
    "                    df.loc[df.shape[0]] = [get_split_name(test_size), dataset, G.number_of_nodes(), G.number_of_edges(), \n",
    "                                           midas_name, method, auc, round(t2-t1, 4)]\n",
    "                    \n",
    "                    time.sleep(20)\n",
    "                    \n",
    "                df.to_csv('./CSV/rav_test_on_splits.csv', index=False)\n",
    "                \n",
    "                time.sleep(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "207bf3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gowalla', 'NYC_Taxi']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASETS[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8a2544b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying a total of 135 combinations.\n",
      "Reading dataset DARPA\n",
      "Processing 9:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Common Neighbours: 100%|███████████████████████████████████████████████████████████| 455k/455k [00:12<00:00, 37.4kit/s]\n",
      "Preferential Attachment: 100%|██████████████████████████████████████████████████████| 455k/455k [00:00<00:00, 456kit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 8:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Common Neighbours: 100%|███████████████████████████████████████████████████████████| 909k/909k [00:26<00:00, 34.0kit/s]\n",
      "Preferential Attachment: 100%|██████████████████████████████████████████████████████| 909k/909k [00:01<00:00, 468kit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 7:3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Common Neighbours: 100%|█████████████████████████████████████████████████████████| 1.37M/1.37M [00:37<00:00, 36.8kit/s]\n",
      "Preferential Attachment: 100%|████████████████████████████████████████████████████| 1.37M/1.37M [00:02<00:00, 512kit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 6:4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Common Neighbours: 100%|█████████████████████████████████████████████████████████| 1.82M/1.82M [00:47<00:00, 38.0kit/s]\n",
      "Preferential Attachment: 100%|████████████████████████████████████████████████████| 1.82M/1.82M [00:03<00:00, 505kit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 5:5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Common Neighbours: 100%|█████████████████████████████████████████████████████████| 2.28M/2.28M [00:50<00:00, 45.1kit/s]\n",
      "Preferential Attachment: 100%|████████████████████████████████████████████████████| 2.28M/2.28M [00:04<00:00, 513kit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 4:6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Common Neighbours: 100%|█████████████████████████████████████████████████████████| 2.73M/2.73M [00:47<00:00, 57.6kit/s]\n",
      "Preferential Attachment: 100%|████████████████████████████████████████████████████| 2.73M/2.73M [00:03<00:00, 711kit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3:7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Common Neighbours: 100%|█████████████████████████████████████████████████████████| 3.19M/3.19M [00:36<00:00, 87.4kit/s]\n",
      "Preferential Attachment: 100%|████████████████████████████████████████████████████| 3.19M/3.19M [00:04<00:00, 731kit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2:8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Common Neighbours: 100%|██████████████████████████████████████████████████████████| 3.64M/3.64M [00:33<00:00, 109kit/s]\n",
      "Preferential Attachment: 100%|████████████████████████████████████████████████████| 3.64M/3.64M [00:05<00:00, 622kit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1:9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Common Neighbours: 100%|██████████████████████████████████████████████████████████| 4.09M/4.09M [00:22<00:00, 179kit/s]\n",
      "Preferential Attachment: 100%|████████████████████████████████████████████████████| 4.09M/4.09M [00:07<00:00, 534kit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset NB15\n",
      "Processing 9:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Common Neighbours: 100%|████████████████████████████████████████████████████████████| 254k/254k [00:00<00:00, 306kit/s]\n",
      "Preferential Attachment: 100%|██████████████████████████████████████████████████████| 254k/254k [00:00<00:00, 518kit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 8:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Common Neighbours: 100%|████████████████████████████████████████████████████████████| 508k/508k [00:01<00:00, 332kit/s]\n",
      "Preferential Attachment: 100%|██████████████████████████████████████████████████████| 508k/508k [00:00<00:00, 547kit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 7:3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Common Neighbours: 100%|████████████████████████████████████████████████████████████| 762k/762k [00:02<00:00, 322kit/s]\n",
      "Preferential Attachment: 100%|██████████████████████████████████████████████████████| 762k/762k [00:01<00:00, 493kit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 6:4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Common Neighbours: 100%|██████████████████████████████████████████████████████████| 1.02M/1.02M [00:02<00:00, 348kit/s]\n",
      "Preferential Attachment: 100%|████████████████████████████████████████████████████| 1.02M/1.02M [00:02<00:00, 484kit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 5:5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Common Neighbours: 100%|██████████████████████████████████████████████████████████| 1.27M/1.27M [00:03<00:00, 326kit/s]\n",
      "Preferential Attachment: 100%|████████████████████████████████████████████████████| 1.27M/1.27M [00:02<00:00, 506kit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 4:6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Common Neighbours: 100%|██████████████████████████████████████████████████████████| 1.52M/1.52M [00:03<00:00, 386kit/s]\n",
      "Preferential Attachment: 100%|████████████████████████████████████████████████████| 1.52M/1.52M [00:02<00:00, 605kit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3:7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Common Neighbours: 100%|██████████████████████████████████████████████████████████| 1.78M/1.78M [00:04<00:00, 365kit/s]\n",
      "Preferential Attachment: 100%|████████████████████████████████████████████████████| 1.78M/1.78M [00:02<00:00, 594kit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2:8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Common Neighbours: 100%|██████████████████████████████████████████████████████████| 2.03M/2.03M [00:05<00:00, 382kit/s]\n",
      "Preferential Attachment: 100%|████████████████████████████████████████████████████| 2.03M/2.03M [00:03<00:00, 622kit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1:9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Common Neighbours: 100%|██████████████████████████████████████████████████████████| 2.29M/2.29M [00:06<00:00, 329kit/s]\n",
      "Preferential Attachment: 100%|████████████████████████████████████████████████████| 2.29M/2.29M [00:04<00:00, 520kit/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset CTU13\n",
      "Processing 9:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Common Neighbours:  73%|████████████████████████████████████████████▋                | 185k/252k [11:32<04:12, 267it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m lp_methods \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCommon Neighbours\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPreferential Attachment\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./CSV/rav_test_on_splits.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m grand_test([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m'\u001b[39m], DATASETS[\u001b[38;5;241m1\u001b[39m:], lp_methods, df\u001b[38;5;241m=\u001b[39mdf)\n",
      "Cell \u001b[1;32mIn[12], line 56\u001b[0m, in \u001b[0;36mgrand_test\u001b[1;34m(midas_list, datasets, lp_methods, df)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m lp_methods\u001b[38;5;241m+\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m'\u001b[39m]: \n\u001b[0;32m     55\u001b[0m     t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 56\u001b[0m     method_score \u001b[38;5;241m=\u001b[39m apply_lp(method, score_test, X_test, G)\n\u001b[0;32m     57\u001b[0m     auc \u001b[38;5;241m=\u001b[39m roc_auc_score(y_test, method_score) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28msum\u001b[39m(y_test) \u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     58\u001b[0m     t2 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32m~\\Desktop\\TUe\\Master Thesis\\thesis_library.py:263\u001b[0m, in \u001b[0;36mapply_lp\u001b[1;34m(method, score, X_test, G)\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;28mlen\u001b[39m(X_test), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m%\u001b[39m(method), unit_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    262\u001b[0m         u, v \u001b[38;5;241m=\u001b[39m X_test[i][\u001b[38;5;241m0\u001b[39m], X_test[i][\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 263\u001b[0m         cn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(G[u])\u001b[38;5;241m.\u001b[39mintersection(\u001b[38;5;28mset\u001b[39m(G[v]))) \u001b[38;5;28;01mif\u001b[39;00m u \u001b[38;5;129;01min\u001b[39;00m G \u001b[38;5;129;01mand\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m G \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    264\u001b[0m         method_score[i] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mcn)\u001b[38;5;241m*\u001b[39mscore[i] \u001b[38;5;28;01mif\u001b[39;00m cn\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m score[i]\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJaccard coefficient\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJaccard Coefficient\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJC\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lp_methods = ['Common Neighbours', 'Preferential Attachment']\n",
    "df = pd.read_csv('./CSV/rav_test_on_splits.csv')\n",
    "grand_test(['None'], DATASETS[4:], lp_methods, df=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e121e1c",
   "metadata": {},
   "source": [
    "## Runtime proofs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e599a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirmation the new splitting works\n",
    "\n",
    "# X, y = read_data('ISCX')\n",
    "# for test_size in [round(0.1*(i+1), 2) for i in range(0, 9)]:\n",
    "#     X_train, X_test, y_train, y_test = split(X, y, test_size)\n",
    "#     print(X_train[-1])\n",
    "#     print(X_test[0])\n",
    "#     print(len(X_test)/(len(X_train)+len(X_test)))\n",
    "#     print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20eb171",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_nx_cn, time_pythonic_cn = [], []\n",
    "time_nx_jc, time_pythonic_jc = [], []\n",
    "time_nx_pa, time_pythonic_pa = [], []\n",
    "\n",
    "for nr_iters in [10000, 50000, 100000, 500000, 1000000]:\n",
    "    \n",
    "    print(\"trying currently nr_iters:\", nr_iters)\n",
    "     G = nx.erdos_renyi_graph(250, 0.03)\n",
    "    \n",
    "    # COMMON NEIGHBOURS:\n",
    "    #Pythonic:\n",
    "    t1 = time.time()\n",
    "    for _ in range(nr_iters):\n",
    "        wow = len(set(G[1]).intersection(set(G[25])))\n",
    "    t2 = time.time()\n",
    "    time_pythonic_cn.append(t2 - t1)\n",
    "    \n",
    "    #Networkx:\n",
    "    t1 = time.time()\n",
    "    for _ in range(nr_iters):\n",
    "        wow = len(tuple(nx.common_neighbors(G, 1, 25)))\n",
    "    t2 = time.time()\n",
    "    time_nx_cn.append(t2 - t1)\n",
    "    \n",
    "    # JACCARD COEFFICIENT:\n",
    "    #Pythonic:\n",
    "    t1 = time.time()\n",
    "    for _ in range(nr_iters):\n",
    "        wow = len(set(G[1]).intersection(set(G[25]))) / len(set(G[1]).union(set(G[25])))\n",
    "    t2 = time.time()\n",
    "    time_pythonic_jc.append(t2 - t1)\n",
    "    \n",
    "    #Networkx:\n",
    "    t1 = time.time()\n",
    "    for _ in range(nr_iters):\n",
    "        wow = next(nx.jaccard_coefficient(G, [(1, 25)]))[2]\n",
    "    t2 = time.time()\n",
    "    time_nx_jc.append(t2 - t1)\n",
    "    \n",
    "    #PREFERENTIAL ATTACHMENT:\n",
    "    # Pythonic:\n",
    "    t1 = time.time()\n",
    "    for _ in range(nr_iters):\n",
    "        wow = len(G[1]) * len(G[25])\n",
    "    t2 = time.time()\n",
    "    time_pythonic_pa.append(t2 - t1)\n",
    "    \n",
    "    # Networkx:\n",
    "    t1 = time.time()\n",
    "    for _ in range(nr_iters):\n",
    "        wow = tuple(nx.preferential_attachment(G, [(1, 25)]))[0][2]\n",
    "    t2 = time.time()\n",
    "    time_nx_pa.append(t2 - t1)\n",
    "    \n",
    "#### RESULTS ####\n",
    "df_time = pd.DataFrame({'size': [10000, 50000, 100000, 500000, 1000000], \n",
    "                        'time_nx': time_nx, 'time_pythonic': time_pythonic})\n",
    "df_time['method'] = ['Jaccard coefficient']*5 + ['Common neighbors']*5 + ['Preferential attachment']*5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee01c39",
   "metadata": {},
   "source": [
    "## Some fuckery about edge ranking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-laptop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict new edges\n",
    "\n",
    "# 1. split dataset\n",
    "# 2. make a graph for training dataset\n",
    "#    - ignore the anomaly edges\n",
    "# 3. process test dataset\n",
    "#    - delete anomaly edges?\n",
    "#    - delete edges already in train dataset\n",
    "#    - delete edges wihich nodes are not in train dataset\n",
    "# 3. for every two nodes in training dataset, compute cn,jc,pa\n",
    "#    - ignore edges already in train dataset\n",
    "#    - output a ranked list\n",
    "# 4. take the first n pairs from ranked list, determine the size of the intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c6b87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def possible_edges_ranking(dataset: str):\n",
    "    '''This code is more-or-less useless due to insane running time\n",
    "    Returns a sorted dataframe of all possible edges and their scores\n",
    "    This might be a massive dataset, so proceed with caution\n",
    "    Does not filter out testing/training edges from the final dataframe\n",
    "    \n",
    "    Converting an iterator into a tuple works (For me) at almost 1M elements per second'''\n",
    "    \n",
    "    data, label = read_data(dataset=dataset)\n",
    "    \n",
    "    train, test, train_label, test_label = split(data, label, test_size = 0.5)\n",
    "\n",
    "    G = construct_training_graph(train, train_label)\n",
    "\n",
    "#     X_test, y_test = filter_test(test, test_label, G)\n",
    "\n",
    "#     #Filtering out the anomalies from the test dataset\n",
    "#     good_rows = [(edge[0], edge[1], label) for (edge, label) in zip(X_test, y_test) if label==0]\n",
    "#     X_test, y_test = [(edge[0], edge[1]) for edge in good_rows], [edge[2] for edge in good_rows]\n",
    "\n",
    "    df = pd.DataFrame(columns=['u', 'v', 'CommonNeighbours','JaccardCoefficient','PreferentialAttachment'])\n",
    "    print(\"Total number of possible edges:\", len(list(combinations(G.nodes,2))))\n",
    "    \n",
    "    #Getting the iterator objects of Jaccard and PrefAtt scores for all pairs of nodes in the G object\n",
    "    jacc = nx.jaccard_coefficient(G)\n",
    "    pref = nx.preferential_attachment(G)\n",
    "    \n",
    "    #Converting these massive generators into tuples might take massive amount of time:\n",
    "    df['JaccardCoefficient'] = tuple(jacc)\n",
    "    df['PreferentialAttachment'] = tuple(pref)\n",
    "    \n",
    "    #And calling the CommonNeighbours on every single pair of nodes might also be super time-consuming:\n",
    "    df['CommonNeighbours'] = df['JaccardCoefficient'].apply(lambda x: len(tuple(nx.common_neighbors(G, x[0], x[1]))))\n",
    "    \n",
    "    #Lastly, retrieve the integer labels of the nodes (might as well be a large computation)\n",
    "    df['u'] = df['JaccardCoefficient'].apply(lambda x: x[0])\n",
    "    df['v'] = df['JaccardCoefficient'].apply(lambda x: x[1])\n",
    "    \n",
    "    df.sort_values(by = ['CommonNeighbours', 'JaccardCoefficient', 'PreferentialAttachment'], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976a2f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0205024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dafc064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_dataset_plot(dataset: str, show_train=True, show_test=True):\n",
    "    '''TO DO: decide whether this is even needed. \n",
    "    IMO it is not, for AUC_on_splits (plot_auc_and_split_per_dataset) does that too, but better'''\n",
    "    \n",
    "    #Using only the proper part of the dataset\n",
    "    df, dataset_info = split_data_reader(dataset)\n",
    "    \n",
    "    reference_values = df[df['Method'] == 'No LP'].reset_index(drop=True)\n",
    "    reference_values = reference_values['AUC'][0], reference_values['AUC'][1]\n",
    "    \n",
    "    plt.figure(figsize=(8,8), dpi=100)\n",
    "    \n",
    "    #The AUC plot:\n",
    "    ax1 = plt.subplot(311)\n",
    "    sns.pointplot(x = 'Split (train:test)', y = 'AUC', hue = 'Method', data = df, ax=ax1)\n",
    "    \n",
    "    ax1.axhline(y = reference_values[0], linestyle = '--') #MIDAS\n",
    "    ax1.axhline(y = reference_values[1], linestyle = '--') #MIDAS-R\n",
    "    \n",
    "    plt.title('ROC/AUC in dataset ' + dataset)\n",
    "    plt.xticks([])\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('AUC/ROC')\n",
    "    \n",
    "    #The anomaly % plot for training data:\n",
    "    if show_train:\n",
    "        ax2 = plt.subplot(312)\n",
    "        sns.pointplot(x = 'Split (train:test)', y = 'Train_anomaly_percentage', \n",
    "                      hue = 'Dataset', data = dataset_info, ax=ax2)\n",
    "        plt.title('Percentage of anomaly edges in training dataset')\n",
    "        plt.ylabel('Anomaly share')\n",
    "        ax2.get_legend().remove();\n",
    "    \n",
    "    #The anomaly % plot for test data:\n",
    "    if show_test:\n",
    "        ax3 = plt.subplot(313)\n",
    "        sns.pointplot(x = 'Split (train:test)', y = 'Test_anomaly_percentage', \n",
    "                      hue = 'Dataset', data = dataset_info, ax=ax3)\n",
    "        plt.title('Percentage of anomaly edges in test dataset')\n",
    "        plt.ylabel('Anomaly share')\n",
    "        ax3.get_legend().remove();\n",
    "    \n",
    "    #plt.savefig(\"Figures/auc_and_anomaly_percentage_\" + df_name + \".png\")\n",
    "    plt.show();\n",
    "    \n",
    "    \n",
    "#single_dataset_plot(\"DARPA\", show_train=True, show_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd469014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_per_method(dataset: str):\n",
    "    '''\n",
    "    dataset: one of DATASETS\n",
    "    '''\n",
    "    \n",
    "    df = pd.read_csv('./CSV/rav_change_s.csv')\n",
    "    df['Size'] = df['Size'].astype(str)\n",
    "    df['Time'] = df['Time'].apply(lambda x: log2(x))\n",
    "    df['Method_full'] = df['Method'] + \" + \" + df['MIDAS']\n",
    "    \n",
    "    plt.figure(figsize=(8,2), dpi=100)\n",
    "    sns.scatterplot(x = 'Size', y = 'Time', data = df[df['Dataset']==dataset], hue='Method_full')\n",
    "    plt.legend(title='')\n",
    "    plt.title('Running time of different sample sizes in dataset ' + dataset)\n",
    "    plt.xlabel(\"Maximum size of the training graph (# unique edges)\")\n",
    "    plt.ylabel(\"log_2(Time) (in seconds)\")\n",
    "    \n",
    "    if dataset + '.png' not in os.listdir('./Figures/Time_vs_sample_size/'):\n",
    "        plt.savefig('./Figures/Time_vs_sample_size/' + dataset + '.png', bbox_inches='tight')\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1,1))\n",
    "    plt.show();\n",
    "    \n",
    "# for dataset in DATASETS:\n",
    "#     plot_time_per_method(df, dataset)\n",
    "\n",
    "plot_time_per_method(df, 'NYC_Taxi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c294fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_test(X_test, y_test, G: nx.Graph) -> tuple:\n",
    "    '''\n",
    "    Filter out edges already present in G\n",
    "    Filter out using a vertex never seen before in G\n",
    "    '''\n",
    "    \n",
    "    good_rows = [(edge[0], edge[1], label) for (edge, label) in zip(X_test, y_test) if \\\n",
    "                 ((edge[0], edge[1]) not in G.edges) and (edge[0] in G and edge[1] in G)]\n",
    "    \n",
    "    print(\"Filtered out\", len(y_test) - len(good_rows), \"rows.\")\n",
    "    print(\"Filtered out\", round((len(y_test) - len(good_rows))/len(y_test)*100, 3), \"% of the original dataset.\")\n",
    "    \n",
    "    return [(edge[0], edge[1]) for edge in good_rows], [edge[2] for edge in good_rows]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
