{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c94894ce",
   "metadata": {},
   "source": [
    "## 1. CSVs created:\n",
    "\n",
    "- ./CSV/dataset_info.csv\n",
    "\n",
    "        For each split of the dataset, lists the exact train and test sizes, as well as the anomaly train and test sizes\n",
    "\n",
    "- ./CSV/test_on_splits.csv\n",
    "\n",
    "        For each split, get the runtime, AUC and other results for running MIDAS with LP (not custom)\n",
    "\n",
    "\n",
    "## 2. Some code to prove the superiority/correctness of our approaches:\n",
    "\n",
    "- Splitting works\n",
    "\n",
    "- sum(y) is faster than y.count(1)\n",
    "\n",
    "- .intersection() or .union() are faster than nx.jaccard_coefficient\n",
    "\n",
    "## 3. Some \"edge ranking\" stuff from Yao's code that seems highly unnecessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "warming-force",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm, trange\n",
    "from Midas import FilteringCore, NormalCore, RelationalCore\n",
    "from random import uniform, randint\n",
    "\n",
    "from thesis_library import *\n",
    "from Custom_sketch import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2204c656",
   "metadata": {},
   "source": [
    "### Create dataset info has simply this single line to invoke:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76861d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_dataset_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52abbe6",
   "metadata": {},
   "source": [
    "### Create rav_test_on_splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4751f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grand_test(midas_list: list, datasets: list, lp_methods: list, df=None) -> None:\n",
    "    '''\n",
    "    1) Runs a given MIDAS (or none) on the ENTIRE dataset\n",
    "    2) Saves the pure MIDAS score on the entire dataset\n",
    "    3) Splits the scores alongside the data to update them with LP scores\n",
    "    4) Updates the scores for each split size and tests all LP methods and None\n",
    "    \n",
    "    If no dataframe is passed, a new one is created and saved at ./CSV/rav_test_on_splits.csv'''\n",
    "    \n",
    "    #TO DO: fix that:\n",
    "    print(\"Trying a total of\", 9*len(midas_list)*len(datasets)*(len(lp_methods)+1), \"combinations.\")\n",
    "    \n",
    "    if df is None and 'test_on_splits.csv' not in os.listdir('./CSV'):\n",
    "        df = pd.DataFrame(columns=['Split (train:test)', 'Dataset', '#nodes', '#edges', 'MIDAS', \n",
    "                                   'Method', 'AUC', 'Runtime'])\n",
    "    \n",
    "    #Handling the dataset loop:\n",
    "    for dataset in datasets:\n",
    "        print(\"Reading dataset\", dataset)\n",
    "        X, y = read_data(dataset, plant='clique')\n",
    "        \n",
    "        #Handling the MIDAS loop:\n",
    "        for midas_name in midas_list:\n",
    "            if midas_name in ['normal', 'Normal', 'MIDAS']:\n",
    "                midas, midas_name = NormalCore(2, 1024), 'MIDAS'       #2048 buckets\n",
    "            elif midas_name in ['relational', 'Relational', 'MIDAS-R']:\n",
    "                midas, midas_name = RelationalCore(2, 1024), 'MIDAS-R' #2048 buckets\n",
    "            elif midas_name in ['custom', 'Custom']:\n",
    "                midas, midas_name = MIDAS(6, 48, 6), 'Custom'          #1728 buckets\n",
    "            elif midas_name in [None, 'None', 'none', 'No sketch']:\n",
    "                midas, midas_name = None, \"No sketch\"\n",
    "            else:\n",
    "                raise ValueError(\"MIDAS version not supported. Pass 'normal', 'relational', 'none'.\")\n",
    "\n",
    "            #Running MIDAS on the entire dataset - the scores will be split\n",
    "            if midas_name in ['MIDAS', 'MIDAS-R']:\n",
    "                score = [0.0] * len(y)\n",
    "                t1 = time.time()\n",
    "                for i in trange(len(y), desc=midas.nameAlg, unit_scale=True):\n",
    "                    score[i] = midas.Call(*X[i])\n",
    "                t2 = time.time()\n",
    "                time_taken = round(t2-t1, 4)\n",
    "                auc = roc_auc_score(y, score)\n",
    "            elif midas_name in ['Custom']:\n",
    "                auc, time_taken, score = midas.process_dataset(dataset, return_score=True, verbose=False)\n",
    "            else:\n",
    "                score, auc, time_taken = [1.0] * len(y), -1, 0\n",
    "            \n",
    "            #Saving the MIDAS score anyway:\n",
    "            df.loc[df.shape[0]] = [\"10:00\", dataset, -1, -1, midas_name, 'No LP', auc, time_taken]\n",
    "            df.to_csv('./CSV/rav_test_on_splits.csv', index=False)\n",
    "\n",
    "            for test_size in [round(0.1*(i+1), 2) for i in range(0, 9)]:\n",
    "\n",
    "                print(\"Processing\", get_split_name(test_size))\n",
    "\n",
    "                #Preparing the split:\n",
    "                X_train, X_test, y_train, y_test, score_test = split(X, y, test_size, score)\n",
    "                G = construct_training_graph(X_train, y_train, True, False) #Not saving anomalies in\n",
    "\n",
    "                #Avoid using no sketch + No LP combo:\n",
    "                if midas_name != 'No sketch':\n",
    "                    lp_methods += ['No LP']\n",
    "                \n",
    "                #Looping over the 3 available LP methods + the None LP method:\n",
    "                for method in lp_methods: \n",
    "\n",
    "                    #print(\"Trying MIDAS version \" + midas_name + \" and LP method:\" + method)\n",
    "                    t1 = time.time()\n",
    "                    method_score = apply_lp(method, score_test, X_test, G)\n",
    "                    auc = roc_auc_score(y_test, method_score) if sum(y_test) !=0 else -1\n",
    "                    t2 = time.time()\n",
    "\n",
    "                    df.loc[df.shape[0]] = [get_split_name(test_size), dataset, G.number_of_nodes(), G.number_of_edges(), \n",
    "                                           midas_name, method, auc, round(t2-t1, 4)]\n",
    "\n",
    "                    time.sleep(20)\n",
    "\n",
    "                df.to_csv('./CSV/test_on_splits.csv', index=False)\n",
    "\n",
    "                time.sleep(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a2544b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./CSV/test_on_splits.csv')\n",
    "\n",
    "#grand_test(['No sketch', 'MIDAS', 'MIDAS-R', 'Custom'], ['Gowalla'], LP_METHODS, df=df)\n",
    "grand_test(['No sketch'], ['NB15'], ['Jaccard Coefficient', 'Common Neighbours'], df=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d7ac6c",
   "metadata": {},
   "source": [
    "# Improvement kind-of calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75492932",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_midas = pd.read_csv('./CSV/test_on_splits.csv')\n",
    "df_midas = df_midas[(df_midas['MIDAS'] == 'MIDAS') | (df_midas['MIDAS'] == 'MIDAS-R')]\n",
    "df_midas = df_midas[(df_midas['Dataset'] != 'ISCX') & (df_midas['#nodes'] == -1)].reset_index(drop=True)\n",
    "df_midas = df_midas.sort_values('MIDAS')\n",
    "\n",
    "df_advice = pd.read_csv('./CSV/rav_grand_test.csv')\n",
    "df_advice = df_advice[(df_advice['Dataset'] != 'ISCX') & (df_advice['K'] == 8)]\n",
    "df_advice = df_advice[df_advice['Split (train:test)'] == '01:09'].reset_index(drop=True)\n",
    "\n",
    "df_midas['AUC_Custom'] = list(df_advice['AUC_sketch']) * 2\n",
    "df_midas['AUC_Advice'] = list(df_advice['AUC_advice']) * 2\n",
    "df_midas = df_midas[['Dataset', 'MIDAS', 'AUC', 'AUC_Custom', 'AUC_Advice']]\n",
    "\n",
    "df_midas['AUC_MIDAS'] = df_midas[df_midas['MIDAS'] == 'MIDAS']['AUC']\n",
    "df_midas['AUC_MIDAS_R'] = df_midas[df_midas['MIDAS'] == 'MIDAS-R']['AUC']\n",
    "df_midas = df_midas.drop(['MIDAS', 'AUC'], axis=1).reset_index(drop=True)\n",
    "df_midas['AUC_MIDAS_R'][:5] = df_midas['AUC_MIDAS_R'][5:]\n",
    "df_midas = df_midas[:5]\n",
    "df_midas = df_midas[['Dataset', 'AUC_MIDAS', 'AUC_MIDAS_R', 'AUC_Custom', 'AUC_Advice']]\n",
    "\n",
    "df_midas['Advice_over_midas'] = (df_midas['AUC_Advice'] - df_midas['AUC_MIDAS'])/df_midas['AUC_MIDAS'] * 100\n",
    "df_midas['Advice_over_midas_r'] = (df_midas['AUC_Advice'] - df_midas['AUC_MIDAS_R'])/df_midas['AUC_MIDAS_R'] * 100\n",
    "\n",
    "print((df_midas['Advice_over_midas'].mean() + df_midas['Advice_over_midas_r'].mean())/2)\n",
    "df_midas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e121e1c",
   "metadata": {},
   "source": [
    "## Runtime proofs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e599a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirmation the new splitting works\n",
    "\n",
    "# X, y = read_data('ISCX')\n",
    "# for test_size in [round(0.1*(i+1), 2) for i in range(0, 9)]:\n",
    "#     X_train, X_test, y_train, y_test = split(X, y, test_size)\n",
    "#     print(X_train[-1])\n",
    "#     print(X_test[0])\n",
    "#     print(len(X_test)/(len(X_train)+len(X_test)))\n",
    "#     print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20eb171",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_nx_cn, time_pythonic_cn = [], []\n",
    "time_nx_jc, time_pythonic_jc = [], []\n",
    "time_nx_pa, time_pythonic_pa = [], []\n",
    "\n",
    "for nr_iters in [10000, 50000, 100000, 500000, 1000000]:\n",
    "    \n",
    "    print(\"trying currently nr_iters:\", nr_iters)\n",
    "     G = nx.erdos_renyi_graph(250, 0.03)\n",
    "    \n",
    "    # COMMON NEIGHBOURS:\n",
    "    #Pythonic:\n",
    "    t1 = time.time()\n",
    "    for _ in range(nr_iters):\n",
    "        wow = len(set(G[1]).intersection(set(G[25])))\n",
    "    t2 = time.time()\n",
    "    time_pythonic_cn.append(t2 - t1)\n",
    "    \n",
    "    #Networkx:\n",
    "    t1 = time.time()\n",
    "    for _ in range(nr_iters):\n",
    "        wow = len(tuple(nx.common_neighbors(G, 1, 25)))\n",
    "    t2 = time.time()\n",
    "    time_nx_cn.append(t2 - t1)\n",
    "    \n",
    "    # JACCARD COEFFICIENT:\n",
    "    #Pythonic:\n",
    "    t1 = time.time()\n",
    "    for _ in range(nr_iters):\n",
    "        wow = len(set(G[1]).intersection(set(G[25]))) / len(set(G[1]).union(set(G[25])))\n",
    "    t2 = time.time()\n",
    "    time_pythonic_jc.append(t2 - t1)\n",
    "    \n",
    "    #Networkx:\n",
    "    t1 = time.time()\n",
    "    for _ in range(nr_iters):\n",
    "        wow = next(nx.jaccard_coefficient(G, [(1, 25)]))[2]\n",
    "    t2 = time.time()\n",
    "    time_nx_jc.append(t2 - t1)\n",
    "    \n",
    "    #PREFERENTIAL ATTACHMENT:\n",
    "    # Pythonic:\n",
    "    t1 = time.time()\n",
    "    for _ in range(nr_iters):\n",
    "        wow = len(G[1]) * len(G[25])\n",
    "    t2 = time.time()\n",
    "    time_pythonic_pa.append(t2 - t1)\n",
    "    \n",
    "    # Networkx:\n",
    "    t1 = time.time()\n",
    "    for _ in range(nr_iters):\n",
    "        wow = tuple(nx.preferential_attachment(G, [(1, 25)]))[0][2]\n",
    "    t2 = time.time()\n",
    "    time_nx_pa.append(t2 - t1)\n",
    "    \n",
    "#### RESULTS ####\n",
    "df_time = pd.DataFrame({'size': [10000, 50000, 100000, 500000, 1000000], \n",
    "                        'time_nx': time_nx, 'time_pythonic': time_pythonic})\n",
    "df_time['method'] = ['Jaccard coefficient']*5 + ['Common neighbors']*5 + ['Preferential attachment']*5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee01c39",
   "metadata": {},
   "source": [
    "## Some fuckery about edge ranking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-laptop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict new edges\n",
    "\n",
    "# 1. split dataset\n",
    "# 2. make a graph for training dataset\n",
    "#    - ignore the anomaly edges\n",
    "# 3. process test dataset\n",
    "#    - delete anomaly edges?\n",
    "#    - delete edges already in train dataset\n",
    "#    - delete edges wihich nodes are not in train dataset\n",
    "# 3. for every two nodes in training dataset, compute cn,jc,pa\n",
    "#    - ignore edges already in train dataset\n",
    "#    - output a ranked list\n",
    "# 4. take the first n pairs from ranked list, determine the size of the intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c6b87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def possible_edges_ranking(dataset: str):\n",
    "    '''This code is more-or-less useless due to insane running time\n",
    "    Returns a sorted dataframe of all possible edges and their scores\n",
    "    This might be a massive dataset, so proceed with caution\n",
    "    Does not filter out testing/training edges from the final dataframe\n",
    "    \n",
    "    Converting an iterator into a tuple works (For me) at almost 1M elements per second'''\n",
    "    \n",
    "    data, label = read_data(dataset=dataset)\n",
    "    \n",
    "    train, test, train_label, test_label = split(data, label, test_size = 0.5)\n",
    "\n",
    "    G = construct_training_graph(train, train_label)\n",
    "\n",
    "#     X_test, y_test = filter_test(test, test_label, G)\n",
    "\n",
    "#     #Filtering out the anomalies from the test dataset\n",
    "#     good_rows = [(edge[0], edge[1], label) for (edge, label) in zip(X_test, y_test) if label==0]\n",
    "#     X_test, y_test = [(edge[0], edge[1]) for edge in good_rows], [edge[2] for edge in good_rows]\n",
    "\n",
    "    df = pd.DataFrame(columns=['u', 'v', 'CommonNeighbours','JaccardCoefficient','PreferentialAttachment'])\n",
    "    print(\"Total number of possible edges:\", len(list(combinations(G.nodes,2))))\n",
    "    \n",
    "    #Getting the iterator objects of Jaccard and PrefAtt scores for all pairs of nodes in the G object\n",
    "    jacc = nx.jaccard_coefficient(G)\n",
    "    pref = nx.preferential_attachment(G)\n",
    "    \n",
    "    #Converting these massive generators into tuples might take massive amount of time:\n",
    "    df['JaccardCoefficient'] = tuple(jacc)\n",
    "    df['PreferentialAttachment'] = tuple(pref)\n",
    "    \n",
    "    #And calling the CommonNeighbours on every single pair of nodes might also be super time-consuming:\n",
    "    df['CommonNeighbours'] = df['JaccardCoefficient'].apply(lambda x: len(tuple(nx.common_neighbors(G, x[0], x[1]))))\n",
    "    \n",
    "    #Lastly, retrieve the integer labels of the nodes (might as well be a large computation)\n",
    "    df['u'] = df['JaccardCoefficient'].apply(lambda x: x[0])\n",
    "    df['v'] = df['JaccardCoefficient'].apply(lambda x: x[1])\n",
    "    \n",
    "    df.sort_values(by = ['CommonNeighbours', 'JaccardCoefficient', 'PreferentialAttachment'], inplace=True)\n",
    "\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
